version: '3.8'

services:
  # OpsFleet Professional ReAct Agent
  opsfleet:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: opsfleet-agent
    restart: unless-stopped
    environment:
      # BigQuery Configuration
      GOOGLE_APPLICATION_CREDENTIALS: /app/credentials/service-account.json
      BIGQUERY_PROJECT_ID: ${BIGQUERY_PROJECT_ID}
      BIGQUERY_DATASET: ${BIGQUERY_DATASET:-thelook_ecommerce}
      
      # LangSmith Tracing
      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}
      LANGSMITH_TRACING: ${LANGSMITH_TRACING:-true}
      LANGSMITH_PROJECT: ${LANGSMITH_PROJECT:-opsfleet}
      LANGSMITH_ENDPOINT: ${LANGSMITH_ENDPOINT:-https://api.smith.langchain.com}
      
      # LLM Provider (Gemini or Ollama)
      GEMINI_API_KEY: ${GEMINI_API_KEY}
      LLM_PROVIDER: ${LLM_PROVIDER:-gemini}
      
      # Ollama Configuration (if using local LLM)
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2}
      
      # Application Configuration
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENABLE_CACHE: ${ENABLE_CACHE:-true}
      CACHE_TTL: ${CACHE_TTL:-3600}
    volumes:
      # Mount credentials
      - ${GOOGLE_APPLICATION_CREDENTIALS:-./credentials}:/app/credentials:ro
      # Mount logs directory
      - ./logs:/app/logs
      # Mount cache directory
      - ./cache:/app/cache
    ports:
      - "8000:8000"
    networks:
      - opsfleet-network
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: ["python", "cli_chat.py", "--verbose"]

  # Ollama - Local LLM Server (optional)
  ollama:
    image: ollama/ollama:latest
    container_name: opsfleet-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - opsfleet-network
    environment:
      OLLAMA_HOST: 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Weaviate - Vector Database (optional, for future features)
  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: opsfleet-weaviate
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      ENABLE_MODULES: 'text2vec-transformers'
      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'
      QUERY_DEFAULTS_LIMIT: 25
      LOG_LEVEL: ${WEAVIATE_LOG_LEVEL:-info}
    volumes:
      - weaviate_data:/var/lib/weaviate
    networks:
      - opsfleet-network
    depends_on:
      - t2v-transformers
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Transformers Inference for Weaviate
  t2v-transformers:
    image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    container_name: opsfleet-transformers
    restart: unless-stopped
    environment:
      ENABLE_CUDA: '0'
    networks:
      - opsfleet-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/.well-known/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  opsfleet-network:
    driver: bridge
    name: opsfleet-network

volumes:
  ollama_data:
    name: opsfleet-ollama-data
  weaviate_data:
    name: opsfleet-weaviate-data
